<article><div class="l"><div class="gg gh gi gj gk gl gm ce gn ch l"></div><div class="l"><header class="pw-post-byline-header go gp gq gr gs gt gu gv gw gx l"><div class="o gy u"><div class="o"><div class="fj l"><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://medium.com/@parano?source=post_page-----c16a4472e2be--------------------------------" rel="noopener follow"><div class="l do"><img alt="Chaoyu Yang" class="l ch fl gz ha fp" height="48" loading="lazy" src="https://miro.medium.com/fit/c/96/96/1*91CzID6eaLN85ymdFPcmfA.jpeg" width="48"/><div class="fk fl l gz ha fo aq"></div></div></a></div><div class="l"><div class="pw-author bm b dm dn ga"><div class="hb o hc"><div><div aria-hidden="false" class="ci"><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://medium.com/@parano?source=post_page-----c16a4472e2be--------------------------------" rel="noopener follow">Chaoyu Yang</a></div></div><div class="hd he hf hg hh d"><span><a class="bm b hi bo hj hk hl hm hn ho hp bd bz hq hr hs cd cf cg ch ci cj" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1ccec70e009f&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdesign-considerations-of-model-deployment-system-c16a4472e2be&amp;user=Chaoyu+Yang&amp;userId=1ccec70e009f&amp;source=post_page-1ccec70e009f----c16a4472e2be---------------------follow_byline-----------" rel="noopener follow">Follow</a></span></div></div></div><div class="o ao ht"><p class="pw-published-date bm b bn bo cn"><span>May 17</span></p><div aria-hidden="true" class="hu ci"><span aria-hidden="true" class="l"><span class="bm b bn bo cn">Â·</span></span></div><div class="pw-reading-time bm b bn bo cn">9 min read</div></div></div></div><div class="o ao"><div class="h k hv hw hx"><div class="hy l fr"><div><div aria-hidden="false" class="ci"><button aria-label="Share on twitter" class="au av aw ax ay az ba bb bc bd be bf bg bh bi"><span class="ci hz dw ia"><svg fill="none" height="24" viewbox="0 0 24 24" width="24"><path d="M20 5.34c-.67.41-1.4.7-2.18.87a3.45 3.45 0 0 0-5.02-.1 3.49 3.49 0 0 0-1.02 2.47c0 .28.03.54.07.8a9.91 9.91 0 0 1-7.17-3.66 3.9 3.9 0 0 0-.5 1.74 3.6 3.6 0 0 0 1.56 2.92 3.36 3.36 0 0 1-1.55-.44V10c0 1.67 1.2 3.08 2.8 3.42-.3.06-.6.1-.94.12l-.62-.06a3.5 3.5 0 0 0 3.24 2.43 7.34 7.34 0 0 1-4.36 1.49l-.81-.05a9.96 9.96 0 0 0 5.36 1.56c6.4 0 9.91-5.32 9.9-9.9v-.5c.69-.49 1.28-1.1 1.74-1.81-.63.3-1.3.48-2 .56A3.33 3.33 0 0 0 20 5.33" fill="#A8A8A8"></path></svg></span></button></div></div></div><div class="hy l fr"><div><div aria-hidden="false" class="ci"><button aria-label="Share on facebook" class="au av aw ax ay az ba bb bc bd be bf bg bh bi"><span class="ci hz dw ia"><svg fill="none" height="24" viewbox="0 0 24 24" width="24"><path d="M19.75 12.04c0-4.3-3.47-7.79-7.75-7.79a7.77 7.77 0 0 0-5.9 12.84 7.77 7.77 0 0 0 4.69 2.63v-5.49h-1.9v-2.2h1.9v-1.62c0-1.88 1.14-2.9 2.8-2.9.8 0 1.49.06 1.69.08v1.97h-1.15c-.91 0-1.1.43-1.1 1.07v1.4h2.17l-.28 2.2h-1.88v5.52a7.77 7.77 0 0 0 6.7-7.71" fill="#A8A8A8"></path></svg></span></button></div></div></div><div class="hy l fr"><div><div aria-hidden="false" class="ci"><button aria-label="Share on linkedin" class="au av aw ax ay az ba bb bc bd be bf bg bh bi"><span class="ci hz dw ia"><svg fill="none" height="24" viewbox="0 0 24 24" width="24"><path d="M19.75 5.39v13.22a1.14 1.14 0 0 1-1.14 1.14H5.39a1.14 1.14 0 0 1-1.14-1.14V5.39a1.14 1.14 0 0 1 1.14-1.14h13.22a1.14 1.14 0 0 1 1.14 1.14zM8.81 10.18H6.53v7.3H8.8v-7.3zM9 7.67a1.31 1.31 0 0 0-1.3-1.32h-.04a1.32 1.32 0 0 0 0 2.64A1.31 1.31 0 0 0 9 7.71v-.04zm8.46 5.37c0-2.2-1.4-3.05-2.78-3.05a2.6 2.6 0 0 0-2.3 1.18h-.07v-1h-2.14v7.3h2.28V13.6a1.51 1.51 0 0 1 1.36-1.63h.09c.72 0 1.26.45 1.26 1.6v3.91h2.28l.02-4.43z" fill="#A8A8A8"></path></svg></span></button></div></div></div><div class="l fr"><div><div aria-hidden="false" class="ci"><button class="au av aw ax ay az ba bb bc bd be bf bg bh bi"><span class="ci hz dw ia"><svg fill="none" height="24" viewbox="0 0 24 24" width="24"><path clip-rule="evenodd" d="M3.57 14.67c0-.57.13-1.11.38-1.6l.02-.02v-.02l.02-.02c0-.02 0-.02.02-.02.12-.26.3-.52.57-.8L7.78 9v-.02l.01-.02c.44-.41.91-.7 1.44-.85a4.87 4.87 0 0 0-1.19 2.36A5.04 5.04 0 0 0 8 11.6L6.04 13.6c-.19.19-.32.4-.38.65a2 2 0 0 0 0 .9c.08.2.2.4.38.57l1.29 1.31c.27.28.62.42 1.03.42.42 0 .78-.14 1.06-.42l1.23-1.25.79-.78 1.15-1.16c.08-.09.19-.22.28-.4.1-.2.15-.42.15-.67 0-.16-.02-.3-.06-.45l-.02-.02v-.02l-.07-.14s0-.03-.04-.06l-.06-.13-.02-.02c0-.02 0-.03-.02-.05a.6.6 0 0 0-.14-.16l-.48-.5c0-.04.02-.1.04-.15l.06-.12 1.17-1.14.09-.09.56.57c.02.04.08.1.16.18l.05.04.03.06.04.05.03.04.04.06.1.14.02.02c0 .02.01.03.03.04l.1.2v.02c.1.16.2.38.3.68a1 1 0 0 1 .04.25 3.2 3.2 0 0 1 .02 1.33 3.49 3.49 0 0 1-.95 1.87l-.66.67-.97.97-1.56 1.57a3.4 3.4 0 0 1-2.47 1.02c-.97 0-1.8-.34-2.49-1.03l-1.3-1.3a3.55 3.55 0 0 1-1-2.51v-.01h-.02v.02zm5.39-3.43c0-.19.02-.4.07-.63.13-.74.44-1.37.95-1.87l.66-.67.97-.98 1.56-1.56c.68-.69 1.5-1.03 2.47-1.03.97 0 1.8.34 2.48 1.02l1.3 1.32a3.48 3.48 0 0 1 1 2.48c0 .58-.11 1.11-.37 1.6l-.02.02v.02l-.02.04c-.14.27-.35.54-.6.8L16.23 15l-.01.02-.01.02c-.44.42-.92.7-1.43.83a4.55 4.55 0 0 0 1.23-3.52L18 10.38c.18-.21.3-.42.35-.65a2.03 2.03 0 0 0-.01-.9 1.96 1.96 0 0 0-.36-.58l-1.3-1.3a1.49 1.49 0 0 0-1.06-.42c-.42 0-.77.14-1.06.4l-1.2 1.27-.8.8-1.16 1.15c-.08.08-.18.21-.29.4a1.66 1.66 0 0 0-.08 1.12l.02.03v.02l.06.14s.01.03.05.06l.06.13.02.02.01.02.01.02c.05.08.1.13.14.16l.47.5c0 .04-.02.09-.04.15l-.06.12-1.15 1.15-.1.08-.56-.56a2.3 2.3 0 0 0-.18-.19c-.02-.01-.02-.03-.02-.04l-.02-.02a.37.37 0 0 1-.1-.12c-.03-.03-.05-.04-.05-.06l-.1-.15-.02-.02-.02-.04-.08-.17v-.02a5.1 5.1 0 0 1-.28-.69 1.03 1.03 0 0 1-.04-.26c-.06-.23-.1-.46-.1-.7v.01z" fill="#A8A8A8" fill-rule="evenodd"></path></svg></span></button></div></div></div><div class="ib o ao"><span><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc16a4472e2be&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdesign-considerations-of-model-deployment-system-c16a4472e2be&amp;source=--------------------------bookmark_header-----------" rel="noopener follow"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" class="au de aw ax ay az ba hz bc id ie if ig"><svg aria-label="Add to list bookmark button" class="ic" fill="none" height="25" viewbox="0 0 25 25" width="25"><path d="M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18V2.5zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V7z" fill="#292929"></path></svg></button></a></span></div></div><div class="ck ih"><div><div aria-hidden="false" class="ci"></div></div></div></div></div><div class="ii ij ik j i d"><div class="fj l"><span><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc16a4472e2be&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdesign-considerations-of-model-deployment-system-c16a4472e2be&amp;source=--------------------------bookmark_header-----------" rel="noopener follow"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" class="au de aw il ay az ba im bc id cd o ao in io ig"><svg aria-label="Add to list bookmark button" class="ic" fill="none" height="25" viewbox="0 0 25 25" width="25"><path d="M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18V2.5zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V7z" fill="#292929"></path></svg><p class="bm b bn bo cn">Save</p></button></a></span></div><div class="ip l fr"><div><div aria-hidden="false" class="ci"><button aria-label="Share on twitter" class="au av aw ax ay az ba bb bc bd be bf bg bh bi"><span class="ci hz dw ia"><svg fill="none" height="24" viewbox="0 0 24 24" width="24"><path d="M20 5.34c-.67.41-1.4.7-2.18.87a3.45 3.45 0 0 0-5.02-.1 3.49 3.49 0 0 0-1.02 2.47c0 .28.03.54.07.8a9.91 9.91 0 0 1-7.17-3.66 3.9 3.9 0 0 0-.5 1.74 3.6 3.6 0 0 0 1.56 2.92 3.36 3.36 0 0 1-1.55-.44V10c0 1.67 1.2 3.08 2.8 3.42-.3.06-.6.1-.94.12l-.62-.06a3.5 3.5 0 0 0 3.24 2.43 7.34 7.34 0 0 1-4.36 1.49l-.81-.05a9.96 9.96 0 0 0 5.36 1.56c6.4 0 9.91-5.32 9.9-9.9v-.5c.69-.49 1.28-1.1 1.74-1.81-.63.3-1.3.48-2 .56A3.33 3.33 0 0 0 20 5.33" fill="#A8A8A8"></path></svg></span></button></div></div></div><div class="ip l fr"><div><div aria-hidden="false" class="ci"><button aria-label="Share on facebook" class="au av aw ax ay az ba bb bc bd be bf bg bh bi"><span class="ci hz dw ia"><svg fill="none" height="24" viewbox="0 0 24 24" width="24"><path d="M19.75 12.04c0-4.3-3.47-7.79-7.75-7.79a7.77 7.77 0 0 0-5.9 12.84 7.77 7.77 0 0 0 4.69 2.63v-5.49h-1.9v-2.2h1.9v-1.62c0-1.88 1.14-2.9 2.8-2.9.8 0 1.49.06 1.69.08v1.97h-1.15c-.91 0-1.1.43-1.1 1.07v1.4h2.17l-.28 2.2h-1.88v5.52a7.77 7.77 0 0 0 6.7-7.71" fill="#A8A8A8"></path></svg></span></button></div></div></div><div class="ip l fr"><div><div aria-hidden="false" class="ci"><button aria-label="Share on linkedin" class="au av aw ax ay az ba bb bc bd be bf bg bh bi"><span class="ci hz dw ia"><svg fill="none" height="24" viewbox="0 0 24 24" width="24"><path d="M19.75 5.39v13.22a1.14 1.14 0 0 1-1.14 1.14H5.39a1.14 1.14 0 0 1-1.14-1.14V5.39a1.14 1.14 0 0 1 1.14-1.14h13.22a1.14 1.14 0 0 1 1.14 1.14zM8.81 10.18H6.53v7.3H8.8v-7.3zM9 7.67a1.31 1.31 0 0 0-1.3-1.32h-.04a1.32 1.32 0 0 0 0 2.64A1.31 1.31 0 0 0 9 7.71v-.04zm8.46 5.37c0-2.2-1.4-3.05-2.78-3.05a2.6 2.6 0 0 0-2.3 1.18h-.07v-1h-2.14v7.3h2.28V13.6a1.51 1.51 0 0 1 1.36-1.63h.09c.72 0 1.26.45 1.26 1.6v3.91h2.28l.02-4.43z" fill="#A8A8A8"></path></svg></span></button></div></div></div><div class="l fr"><div><div aria-hidden="false" class="ci"><button class="au av aw ax ay az ba bb bc bd be bf bg bh bi"><span class="ci hz dw ia"><svg fill="none" height="24" viewbox="0 0 24 24" width="24"><path clip-rule="evenodd" d="M3.57 14.67c0-.57.13-1.11.38-1.6l.02-.02v-.02l.02-.02c0-.02 0-.02.02-.02.12-.26.3-.52.57-.8L7.78 9v-.02l.01-.02c.44-.41.91-.7 1.44-.85a4.87 4.87 0 0 0-1.19 2.36A5.04 5.04 0 0 0 8 11.6L6.04 13.6c-.19.19-.32.4-.38.65a2 2 0 0 0 0 .9c.08.2.2.4.38.57l1.29 1.31c.27.28.62.42 1.03.42.42 0 .78-.14 1.06-.42l1.23-1.25.79-.78 1.15-1.16c.08-.09.19-.22.28-.4.1-.2.15-.42.15-.67 0-.16-.02-.3-.06-.45l-.02-.02v-.02l-.07-.14s0-.03-.04-.06l-.06-.13-.02-.02c0-.02 0-.03-.02-.05a.6.6 0 0 0-.14-.16l-.48-.5c0-.04.02-.1.04-.15l.06-.12 1.17-1.14.09-.09.56.57c.02.04.08.1.16.18l.05.04.03.06.04.05.03.04.04.06.1.14.02.02c0 .02.01.03.03.04l.1.2v.02c.1.16.2.38.3.68a1 1 0 0 1 .04.25 3.2 3.2 0 0 1 .02 1.33 3.49 3.49 0 0 1-.95 1.87l-.66.67-.97.97-1.56 1.57a3.4 3.4 0 0 1-2.47 1.02c-.97 0-1.8-.34-2.49-1.03l-1.3-1.3a3.55 3.55 0 0 1-1-2.51v-.01h-.02v.02zm5.39-3.43c0-.19.02-.4.07-.63.13-.74.44-1.37.95-1.87l.66-.67.97-.98 1.56-1.56c.68-.69 1.5-1.03 2.47-1.03.97 0 1.8.34 2.48 1.02l1.3 1.32a3.48 3.48 0 0 1 1 2.48c0 .58-.11 1.11-.37 1.6l-.02.02v.02l-.02.04c-.14.27-.35.54-.6.8L16.23 15l-.01.02-.01.02c-.44.42-.92.7-1.43.83a4.55 4.55 0 0 0 1.23-3.52L18 10.38c.18-.21.3-.42.35-.65a2.03 2.03 0 0 0-.01-.9 1.96 1.96 0 0 0-.36-.58l-1.3-1.3a1.49 1.49 0 0 0-1.06-.42c-.42 0-.77.14-1.06.4l-1.2 1.27-.8.8-1.16 1.15c-.08.08-.18.21-.29.4a1.66 1.66 0 0 0-.08 1.12l.02.03v.02l.06.14s.01.03.05.06l.06.13.02.02.01.02.01.02c.05.08.1.13.14.16l.47.5c0 .04-.02.09-.04.15l-.06.12-1.15 1.15-.1.08-.56-.56a2.3 2.3 0 0 0-.18-.19c-.02-.01-.02-.03-.02-.04l-.02-.02a.37.37 0 0 1-.1-.12c-.03-.03-.05-.04-.05-.06l-.1-.15-.02-.02-.02-.04-.08-.17v-.02a5.1 5.1 0 0 1-.28-.69 1.03 1.03 0 0 1-.04-.26c-.06-.23-.1-.46-.1-.7v.01z" fill="#A8A8A8" fill-rule="evenodd"></path></svg></span></button></div></div></div></div></header><span class="l"></span><section><div><div class="fo as iv iw ix iy"></div><div class="iz ja jb jc jd"><div class=""><h1 class="pw-post-title je jf jg bm jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc ga" id="faee">Design Considerations for Model Deployment Systems</h1></div><div class=""><h2 class="pw-subtitle-paragraph kd jf jg bm b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku cn" id="b90a"><strong class="ba">An engineerâ€™s guide to understanding cross-functional requirements for deploying Machine-Learning Models</strong></h2></div><figure class="kw kx ky kz gx la gl gm paragraph-image"><div class="lb lc do ld ce le" role="button" tabindex="0"><div class="gl gm kv"><picture><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/max/640/1*Z3GT4j_Y-KxNhFJ9OnJJiA.jpeg 640w, https://miro.medium.com/max/720/1*Z3GT4j_Y-KxNhFJ9OnJJiA.jpeg 720w, https://miro.medium.com/max/750/1*Z3GT4j_Y-KxNhFJ9OnJJiA.jpeg 750w, https://miro.medium.com/max/786/1*Z3GT4j_Y-KxNhFJ9OnJJiA.jpeg 786w, https://miro.medium.com/max/828/1*Z3GT4j_Y-KxNhFJ9OnJJiA.jpeg 828w, https://miro.medium.com/max/1100/1*Z3GT4j_Y-KxNhFJ9OnJJiA.jpeg 1100w, https://miro.medium.com/max/1400/1*Z3GT4j_Y-KxNhFJ9OnJJiA.jpeg 1400w"/><img alt="Photo by Kelly Sikkema on Unsplash" class="ce lf lg c" height="467" loading="eager" width="700"/></picture></div></div><figcaption class="lh bl gn gl gm li lj bm b bn bo cn">Photo by <a class="au lk" href="https://unsplash.com/@kellysikkema" rel="noopener ugc nofollow" target="_blank">Kelly Sikkema</a> on <a class="au lk" href="https://unsplash.com" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p class="pw-post-body-paragraph ll lm jg ln b lo lp kh lq lr ls kk lt lu lv lw lx ly lz ma mb mc md me mf mg iz ga" id="918c">The impact of Machine learning is becoming more widespread each day, powering applications like product recommendations, fraud detection, and conversational AI. Data Science teams are no longer just sharing business insights with decision-makers via dashboards or presentations. More often, only by putting the models into end applications, the full potential of ML can be realized.</p><p class="pw-post-body-paragraph ll lm jg ln b lo lp kh lq lr ls kk lt lu lv lw lx ly lz ma mb mc md me mf mg iz ga" id="8e2f">However, deploying machine learning models remains one of the most time-consuming engineering challenges. As the engineer tasked with building the model deployment pipeline, itâ€™s critical to understand that this is a cross-functional effort and requires collaborating with multiple organizational stakeholders.</p><p class="pw-post-body-paragraph ll lm jg ln b lo lp kh lq lr ls kk lt lu lv lw lx ly lz ma mb mc md me mf mg iz ga" id="294f">In this article, we will dive into the key requirements of model deployment systems, coming from three stakeholders: the Data Science team, the DevOps team and the Product team. Then, weâ€™ll breakdown the main design considerations (marked with ðŸ’¡) in order to meet those requirements, alongside with practical recommendations.</p><h1 class="mh mi jg bm mj mk ml mm mn mo mp mq mr km ms kn mt kp mu kq mv ks mw kt mx my ga" id="056a">The Data Scientist</h1><p class="pw-post-body-paragraph ll lm jg ln b lo mz kh lq lr na kk lt lu nb lw lx ly nc ma mb mc nd me mf mg iz ga" id="af3a">The Data Science team may be the team with the most (and likely most challenging) requirements to take into consideration â€” after all, itâ€™s their models youâ€™re bringing to fruition. Their primary goal is to <strong class="ln jh">build accurate and impactful models</strong>.</p><h2 class="ne mi jg bm mj nf ng nh mn ni nj nk mr lu nl nm mt ly nn no mv mc np nq mx nr ga" id="679b">#1: Seamless transition from training environments</h2><p class="pw-post-body-paragraph ll lm jg ln b lo mz kh lq lr na kk lt lu nb lw lx ly nc ma mb mc nd me mf mg iz ga" id="06c4">Data Scientists develop their models with specialized development tools, such as Jupyter notebook, different type of ML training frameworks and experimentation management platforms â€” environments that may be unfamiliar to many software engineers. This made the transition of model artifacts and feature extraction code to production one of the most error-prone processes in the deployment pipeline.</p><blockquote class="ns nt nu"><p class="ll lm nv ln b lo lp kh lq lr ls kk lt nw lv lw lx nx lz ma mb ny md me mf mg iz ga" id="84a2"><strong class="ln jh"><em class="jg">ðŸ’¡</em></strong><em class="jg"> A mature model deployment system must be able to easily integrate with multiple ML frameworks and model development environments, giving data science teams the flexibility to choose any tools they found suitable for their model training needs.</em></p></blockquote><h2 class="ne mi jg bm mj nf ng nh mn ni nj nk mr lu nl nm mt ly nn no mv mc np nq mx nr ga" id="d936">#2: The ability to continuously redeploy new models</h2><p class="pw-post-body-paragraph ll lm jg ln b lo mz kh lq lr na kk lt lu nb lw lx ly nc ma mb mc nd me mf mg iz ga" id="ae17">Data Science teams understand that models donâ€™t remain static after being deployed to production â€” concept drift and data drift may occur over time, requiring retraining and updating new models periodically. Your Data Science team may also want to incorporate new features, update model architectures, or experiment with new ML frameworks. Deployment systems should allow data science teams to easily update new models in production, and do so with confidence.</p><blockquote class="ns nt nu"><p class="ll lm nv ln b lo lp kh lq lr ls kk lt nw lv lw lx nx lz ma mb ny md me mf mg iz ga" id="e10c"><strong class="ln jh"><em class="jg">ðŸ’¡</em></strong><em class="jg"> Youâ€™ll need a way to manage the code, as well as the model and library dependencies all together. In addition, all of these pieces must be versioned and deployed together for reproducibility and a smooth rollback (if necessary).</em></p></blockquote><p class="pw-post-body-paragraph ll lm jg ln b lo lp kh lq lr ls kk lt lu lv lw lx ly lz ma mb mc md me mf mg iz ga" id="ddf2">This requirement also introduces a versioning challenge: the more models that are shipped, the harder it is to keep track of important details like how each model was trained, who has access, and who is responsible for monitoring to ensure quality.</p><blockquote class="ns nt nu"><p class="ll lm nv ln b lo lp kh lq lr ls kk lt nw lv lw lx nx lz ma mb ny md me mf mg iz ga" id="17b6"><strong class="ln jh"><em class="jg">ðŸ’¡ </em></strong><em class="jg">A model registry to record and recall specific details will simplify operational overhead related to managing multiple versions of a model.</em></p></blockquote><h2 class="ne mi jg bm mj nf ng nh mn ni nj nk mr lu nl nm mt ly nn no mv mc np nq mx nr ga" id="96c7">#3: Visibility of model performance</h2><p class="pw-post-body-paragraph ll lm jg ln b lo mz kh lq lr na kk lt lu nb lw lx ly nc ma mb mc nd me mf mg iz ga" id="2100">The ability to understand how a model is performing once itâ€™s live is critical. Consider this example: When a new model for recommending products results in <em class="nv">fewer</em> sales than before, it may be a sign that the new modelâ€™s predictions are less accurate. Your system should help to detect that and roll back to a previous version as soon as posssible.</p><p class="pw-post-body-paragraph ll lm jg ln b lo lp kh lq lr ls kk lt lu lv lw lx ly lz ma mb mc md me mf mg iz ga" id="3bc9">Concept and data drift also occur over time as habits and trends change. When this happens, data science teams need to investigate to determine if a model needs to be retrained.</p><blockquote class="ns nt nu"><p class="ll lm nv ln b lo lp kh lq lr ls kk lt nw lv lw lx nx lz ma mb ny md me mf mg iz ga" id="6f14"><strong class="ln jh"><em class="jg">ðŸ’¡</em></strong><em class="jg"> Your ML serving solution should make it easy for data scientists to retrieve and analyze predictions in real-time. Simple solutions like CloudWatch, DataDog or Grafana Loki may suffice for certain cases, but more complex cases may require specialized ML monitoring solutions from companies like WhyLabs, Fiddler.ai, or Arize.</em></p></blockquote><h2 class="ne mi jg bm mj nf ng nh mn ni nj nk mr lu nl nm mt ly nn no mv mc np nq mx nr ga" id="9a55">#4: Easy access to features</h2><p class="pw-post-body-paragraph ll lm jg ln b lo mz kh lq lr na kk lt lu nb lw lx ly nc ma mb mc nd me mf mg iz ga" id="dcb9">The viability of an ML model is not only dependent on its raw input data, but also on the transforms that create additional derived features. Many times in online prediction scenarios these transformations also need to be performed in real-time as the data is being ingested. The key challenges are defining a common interface for retrieving feature data as well as handling feature transformation in the same manner for both training and online serving.</p><blockquote class="ns nt nu"><p class="ll lm nv ln b lo lp kh lq lr ls kk lt nw lv lw lx nx lz ma mb ny md me mf mg iz ga" id="05fa"><strong class="ln jh"><em class="jg">ðŸ’¡ </em></strong><em class="jg">For most feature transformations, embedding the code as part of the serving pipeline directly is preferred as it ensures the model and feature processing code are always on the same version. However when working with features that requires online aggregation over time-series data, computing such features with low-latency is challenging. For this type of use cases, you may consider using a dedicated feature store, that exposes a common interface for both the online serving and the training pipelines to access features. A good feature store can also help coordinate rollouts of new features or a new version of a feature, so that it is synchronized with your models.</em></p></blockquote><h1 class="mh mi jg bm mj mk ml mm mn mo mp mq mr km ms kn mt kp mu kq mv ks mw kt mx my ga" id="809e">The DevOps Engineer</h1><p class="pw-post-body-paragraph ll lm jg ln b lo mz kh lq lr na kk lt lu nb lw lx ly nc ma mb mc nd me mf mg iz ga" id="9d31">Youâ€™ll likely be working very closely with the DevOps team to deploy ML models. This team is primarily <strong class="ln jh">responsible for the underlying technical infrastructure </strong>that powers your companyâ€™s software products, overseeing critical areas like the stability and security of the whole operation.</p><h2 class="ne mi jg bm mj nf ng nh mn ni nj nk mr lu nl nm mt ly nn no mv mc np nq mx nr ga" id="0397">#1: Deploying models on preferred platforms</h2><p class="pw-post-body-paragraph ll lm jg ln b lo mz kh lq lr na kk lt lu nb lw lx ly nc ma mb mc nd me mf mg iz ga" id="1d65">Model serving workloads usually rely on other existing services managed by the DevOps team for provisioning resources, retrieving features, streaming logs, and monitoring. With specific cloud or on-premise environments, deployment patterns, and tooling already set up, itâ€™ll be important to deploy new ML services in a similar way to ease the maintenance burden.</p><blockquote class="ns nt nu"><p class="ll lm nv ln b lo lp kh lq lr ls kk lt nw lv lw lx nx lz ma mb ny md me mf mg iz ga" id="a893"><strong class="ln jh"><em class="jg">ðŸ’¡</em></strong><em class="jg"> Your prediction service should be able to work with different logging, monitoring, and provisioning tools. Additionally, it should be optimized to run in many different forms depending on your use case â€” whether itâ€™s as a real-time service running on Kubernetes, a batch inference job with Spark, or a serverless deployment in the cloud, provisioned via Terraform or CloudFormation.</em></p></blockquote><h2 class="ne mi jg bm mj nf ng nh mn ni nj nk mr lu nl nm mt ly nn no mv mc np nq mx nr ga" id="b751">#2: Ensuring reliability and maintenance of the service</h2><p class="pw-post-body-paragraph ll lm jg ln b lo mz kh lq lr na kk lt lu nb lw lx ly nc ma mb mc nd me mf mg iz ga" id="5933">Once the model is deployed, the reliability of the service is extremely important to the DevOps team.</p><blockquote class="ns nt nu"><p class="ll lm nv ln b lo lp kh lq lr ls kk lt nw lv lw lx nx lz ma mb ny md me mf mg iz ga" id="594c"><strong class="ln jh"><em class="jg">ðŸ’¡</em></strong><em class="jg"> DevOps teams should have the ability to monitor, reproduce identical deployments to create testing environments, and incorporate CI testing.</em></p></blockquote><p class="pw-post-body-paragraph ll lm jg ln b lo lp kh lq lr ls kk lt lu lv lw lx ly lz ma mb mc md me mf mg iz ga" id="dca1"><strong class="ln jh">For monitoring, tracing, and alerting:</strong> Model serving workloads should make it easy to integrate with systems like Prometheus, OpenTelemetry, CloudWatch, or Datadog. This allows the DevOps team to monitor these new ML services in the same way they monitor other existing services.</p><p class="pw-post-body-paragraph ll lm jg ln b lo lp kh lq lr ls kk lt lu lv lw lx ly lz ma mb mc md me mf mg iz ga" id="2810"><strong class="ln jh">For reproducibility:</strong> To minimize the chances of failed deployments in the future, the system should be able to reproduce identical deployments to create testing environments. Being able to easily revert all systems to a previous state in the case of a production outage gives the DevOps team peace of mind. The GitOps flow is a standard, declarative way of describing the desired state of a deployment and automatically reproducing the entire stack if needed.</p><p class="pw-post-body-paragraph ll lm jg ln b lo lp kh lq lr ls kk lt lu lv lw lx ly lz ma mb mc md me mf mg iz ga" id="9da6">E.g. the code sample below shows a model deployment specification on Kubernetes with Yatai and BentoML:</p><figure class="kw kx ky kz gx la"><div class="m fs l do"><div class="nz oa l"></div></div></figure><p class="pw-post-body-paragraph ll lm jg ln b lo lp kh lq lr ls kk lt lu lv lw lx ly lz ma mb mc md me mf mg iz ga" id="8817"><strong class="ln jh">For CI and testing:</strong> Unexpected behavior due to changes in the serving logic or model prediction can be extremely disruptive, which is why CI testing is a must-have to catch issues before they hit production.</p><blockquote class="ns nt nu"><p class="ll lm nv ln b lo lp kh lq lr ls kk lt nw lv lw lx nx lz ma mb ny md me mf mg iz ga" id="8fec"><strong class="ln jh"><em class="jg">ðŸ’¡</em></strong><em class="jg"> Your model deployment solution should allow for integration with CI pipelines that can test the entire serving pipeline. Inline automated tests should not only evaluate business logic in the code but also verify that key model prediction metrics have not drifted out of acceptable bounds.</em></p></blockquote><h2 class="ne mi jg bm mj nf ng nh mn ni nj nk mr lu nl nm mt ly nn no mv mc np nq mx nr ga" id="1d59">#3: Securing access to the model</h2><p class="pw-post-body-paragraph ll lm jg ln b lo mz kh lq lr na kk lt lu nb lw lx ly nc ma mb mc nd me mf mg iz ga" id="3da5">Securing access to ML models is equally important as securing access to other critical parts of the business, like customer data or your companyâ€™s IP.</p><blockquote class="ns nt nu"><p class="ll lm nv ln b lo lp kh lq lr ls kk lt nw lv lw lx nx lz ma mb ny md me mf mg iz ga" id="0764"><strong class="ln jh"><em class="jg">ðŸ’¡</em></strong><em class="jg"> ML prediction services, just like any other online service, should apply common security </em><a class="au lk" href="https://owasp.org/www-project-top-ten/" rel="noopener ugc nofollow" target="_blank"><em class="jg">best practices</em></a><em class="jg"> with regards to authentication and access. We see many users deploying prediction services behind Istio. This type of service mesh can be configured for more complex authentication and routing scenarios. Itâ€™s also worth noting that ML services in particular can present new types of security challenges, like </em><a class="au lk" href="https://openai.com/blog/adversarial-example-research/" rel="noopener ugc nofollow" target="_blank"><em class="jg">adversarial examples</em></a><em class="jg">, intended to cause models to make mistakes.</em></p></blockquote><h1 class="mh mi jg bm mj mk ml mm mn mo mp mq mr km ms kn mt kp mu kq mv ks mw kt mx my ga" id="348f">The Product Manager</h1><p class="pw-post-body-paragraph ll lm jg ln b lo mz kh lq lr na kk lt lu nb lw lx ly nc ma mb mc nd me mf mg iz ga" id="bd97">Your product team is an important stakeholder â€” after all, theyâ€™re the ones not only responsible for MLâ€™s business impact, but also all of the economic, social and legal issues around ML. Their considerations for ML deployment systems mainly fall in the camps of <strong class="ln jh">proving the value of the model </strong>and <strong class="ln jh">delivering a final product with a great user experience</strong>.</p><p class="pw-post-body-paragraph ll lm jg ln b lo lp kh lq lr ls kk lt lu lv lw lx ly lz ma mb mc md me mf mg iz ga" id="3f11"><em class="nv">Note: product manager here generally means product owner â€” whoever in your organization that is in charge of the success of the ML project and constantly advocate for end-users as well as business value.</em></p><h2 class="ne mi jg bm mj nf ng nh mn ni nj nk mr lu nl nm mt ly nn no mv mc np nq mx nr ga" id="153e">#1: Quickly validate the value of the model</h2><p class="pw-post-body-paragraph ll lm jg ln b lo mz kh lq lr na kk lt lu nb lw lx ly nc ma mb mc nd me mf mg iz ga" id="27ef">The product team will want to bring a functioning prototype to market as soon as possible in order to validate its value. Because of this time-to-market requirement, many ML experts believe ML projects should start on a smaller scale at first, then adjust and refine over time.</p><blockquote class="ns nt nu"><p class="ll lm nv ln b lo lp kh lq lr ls kk lt nw lv lw lx nx lz ma mb ny md me mf mg iz ga" id="81dc"><strong class="ln jh"><em class="jg">ðŸ’¡ </em></strong><em class="jg">When deploying a model to production, skip the more advanced features initially. When the model seems viable, everything can be fine-tuned for the best results.</em></p></blockquote><h2 class="ne mi jg bm mj nf ng nh mn ni nj nk mr lu nl nm mt ly nn no mv mc np nq mx nr ga" id="09f8">#2: Meet the latency requirements</h2><p class="pw-post-body-paragraph ll lm jg ln b lo mz kh lq lr na kk lt lu nb lw lx ly nc ma mb mc nd me mf mg iz ga" id="6a4a">Prediction latency is a key consideration for the Product team because it has a direct impact on the end userâ€™s experience (and therefore, on revenue). Consider this example: A video game company using an ML model to match competing players with similar skills must be near instantaneous â€” if it runs too slowly, it could result in frustrated users quitting.</p><blockquote class="ns nt nu"><p class="ll lm nv ln b lo lp kh lq lr ls kk lt nw lv lw lx nx lz ma mb ny md me mf mg iz ga" id="78a7"><strong class="ln jh"><em class="jg">ðŸ’¡</em></strong><em class="jg"> Depending on the use case, there may be different latency SLAs (Service Level Agreement) that the model serving system needs to take into consideration. For some time-sensitive and mission critical use cases, like ad targeting, the ML serving system needs to deliver a prediction in real-time with P99 &lt; 50ms latency. For others, like a fraud detection model for issuing credit cards, a few minutes will suffice.</em></p></blockquote><h2 class="ne mi jg bm mj nf ng nh mn ni nj nk mr lu nl nm mt ly nn no mv mc np nq mx nr ga" id="a6ad">#3: Reduce operational costs</h2><p class="pw-post-body-paragraph ll lm jg ln b lo mz kh lq lr na kk lt lu nb lw lx ly nc ma mb mc nd me mf mg iz ga" id="0623">Large scale machine learning is extremely compute-intensive workload and involves potentially thousands of machines operating at the same time. This comes with cost in terms of energy, maintaining, and cloud spendings for your organizations. A ML model only make sense for a business, if the value it delivers outweighs the cost of running it. Thatâ€™s why product teams want to assure the value a model delivers, while optimizing for cost.</p><blockquote class="ns nt nu"><p class="ll lm nv ln b lo lp kh lq lr ls kk lt nw lv lw lx nx lz ma mb ny md me mf mg iz ga" id="8680"><strong class="ln jh"><em class="jg">ðŸ’¡</em></strong><em class="jg"> A solution that optimizes for horizontal and vertical scaling out-of-the-box ensures that costs are minimized at the start. Different use cases may require one or the other.</em></p></blockquote><p class="pw-post-body-paragraph ll lm jg ln b lo lp kh lq lr ls kk lt lu lv lw lx ly lz ma mb mc md me mf mg iz ga" id="515d"><strong class="ln jh">Vertical scale and resource utilization: </strong>If prediction traffic requires specialized hardware acceleration or can be more cost-effective by scaling vertically, youâ€™ll want to make sure that <strong class="ln jh">the software is optimized for the hardware youâ€™ve chosen</strong>. Specifically, youâ€™ll want to make sure it supports common performance best practices such as <a class="au lk" href="https://rise.cs.berkeley.edu/wp-content/uploads/2017/02/clipper_final.pdf" rel="noopener ugc nofollow" target="_blank">adaptive batching of prediction requests</a>, properly distributing load to all CPU cores available, and utilizing GPUs or custom accelerators where needed. This not only helps minimize latency for individual requests, but reduces the amount of compute resources required by utilizing resources efficiently.</p><p class="pw-post-body-paragraph ll lm jg ln b lo lp kh lq lr ls kk lt lu lv lw lx ly lz ma mb mc md me mf mg iz ga" id="fbce"><strong class="ln jh">Horizontal Auto-scaling:</strong> If prediction traffic varies throughout a specified time period (like a food delivery service with surges during lunch time or event-related spikes around Black Friday and Christmas, for example), the solution should be able to automatically scale horizontally in order to balance latency requirements and resource utilization.</p><p class="pw-post-body-paragraph ll lm jg ln b lo lp kh lq lr ls kk lt lu lv lw lx ly lz ma mb mc md me mf mg iz ga" id="77ef"><strong class="ln jh">Serverless and scale-to-zero:</strong> If prediction traffic is limited to just a handful of predictions each month, you may want to consider a serverless architecture like <a class="au lk" href="https://cloud.google.com/knative" rel="noopener ugc nofollow" target="_blank">Knative</a> or <a class="au lk" href="https://aws.amazon.com/lambda/" rel="noopener ugc nofollow" target="_blank">AWS Lambda</a>. These solutions can <strong class="ln jh">scale down to 0</strong> when no requests are in progress, saving on cost.</p><h1 class="mh mi jg bm mj mk ml mm mn mo mp mq mr km ms kn mt kp mu kq mv ks mw kt mx my ga" id="516e">Conclusion</h1><p class="pw-post-body-paragraph ll lm jg ln b lo mz kh lq lr na kk lt lu nb lw lx ly nc ma mb mc nd me mf mg iz ga" id="47b1">Data Science, DevOps, and Product are all teams that have a stake in the success of an ML project. The diversity of needs across these teams creates challenges that are very different than the typical software deployment process. While it may not be necessary to address all of these considerations in the very first iteration of an ML model deployment system, it is helpful to know what may lay ahead so that proper planning and research can be done to ensure the greatest likelihood of success.</p><p class="pw-post-body-paragraph ll lm jg ln b lo lp kh lq lr ls kk lt lu lv lw lx ly lz ma mb mc md me mf mg iz ga" id="c160"><em class="nv">A bit background: my name is Chaoyu Yang, Iâ€™m the creator of the open source model serving framework </em><a class="au lk" href="https://github.com/bentoml/BentoML" rel="noopener ugc nofollow" target="_blank"><em class="nv">BentoML</em></a><em class="nv">. Previously, I helped to create Databricksâ€™ unified data science platform in its early days. If you are interested in learning more about this topic or be part of the conversations, join our </em><a class="au lk" href="https://link.bentoml.com/44cb" rel="noopener ugc nofollow" target="_blank"><em class="nv">community slack</em></a><em class="nv"> where hundreds of ML practitioners gathered and discuss all things MLOps.</em></p></div></div></section></div></div></article>