<article><div class="l"><div class="gg gh gi gj gk gl gm ce gn ch l"></div><div class="l"><header class="pw-post-byline-header go gp gq gr gs gt gu gv gw gx l"><div class="o gy u"><div class="o"><div class="fj l"><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://medium.com/@michaelpotter?source=post_page-----ba70df6957d--------------------------------" rel="noopener follow"><div class="l do"><img alt="Michael Potter" class="l ch fl gz ha fp" height="48" loading="lazy" src="https://miro.medium.com/fit/c/96/96/1*14PpTkwzeaibnPN6STu4_Q.jpeg" width="48"/><div class="fk fl l gz ha fo aq"></div></div></a></div><div class="l"><div class="pw-author bm b dm dn ga"><div class="hb o hc"><div><div aria-hidden="false" class="ci"><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://medium.com/@michaelpotter?source=post_page-----ba70df6957d--------------------------------" rel="noopener follow">Michael Potter</a></div></div><div class="hd he hf hg hh d"><span><a class="bm b hi bo hj hk hl hm hn ho hp bd bz hq hr hs cd cf cg ch ci cj" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6a1348462387&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-ml-models-that-are-useless-ba70df6957d&amp;user=Michael+Potter&amp;userId=6a1348462387&amp;source=post_page-6a1348462387----ba70df6957d---------------------follow_byline-----------" rel="noopener follow">Follow</a></span></div></div></div><div class="o ao ht"><p class="pw-published-date bm b bn bo cn"><span>Feb 15</span></p><div aria-hidden="true" class="hu ci"><span aria-hidden="true" class="l"><span class="bm b bn bo cn">·</span></span></div><div class="pw-reading-time bm b bn bo cn">9 min read</div></div></div></div><div class="o ao"><div class="h k hv hw hx"><div class="hy l fr"><div><div aria-hidden="false" class="ci"><button aria-label="Share on twitter" class="au av aw ax ay az ba bb bc bd be bf bg bh bi"><span class="ci hz dw ia"><svg fill="none" height="24" viewbox="0 0 24 24" width="24"><path d="M20 5.34c-.67.41-1.4.7-2.18.87a3.45 3.45 0 0 0-5.02-.1 3.49 3.49 0 0 0-1.02 2.47c0 .28.03.54.07.8a9.91 9.91 0 0 1-7.17-3.66 3.9 3.9 0 0 0-.5 1.74 3.6 3.6 0 0 0 1.56 2.92 3.36 3.36 0 0 1-1.55-.44V10c0 1.67 1.2 3.08 2.8 3.42-.3.06-.6.1-.94.12l-.62-.06a3.5 3.5 0 0 0 3.24 2.43 7.34 7.34 0 0 1-4.36 1.49l-.81-.05a9.96 9.96 0 0 0 5.36 1.56c6.4 0 9.91-5.32 9.9-9.9v-.5c.69-.49 1.28-1.1 1.74-1.81-.63.3-1.3.48-2 .56A3.33 3.33 0 0 0 20 5.33" fill="#A8A8A8"></path></svg></span></button></div></div></div><div class="hy l fr"><div><div aria-hidden="false" class="ci"><button aria-label="Share on facebook" class="au av aw ax ay az ba bb bc bd be bf bg bh bi"><span class="ci hz dw ia"><svg fill="none" height="24" viewbox="0 0 24 24" width="24"><path d="M19.75 12.04c0-4.3-3.47-7.79-7.75-7.79a7.77 7.77 0 0 0-5.9 12.84 7.77 7.77 0 0 0 4.69 2.63v-5.49h-1.9v-2.2h1.9v-1.62c0-1.88 1.14-2.9 2.8-2.9.8 0 1.49.06 1.69.08v1.97h-1.15c-.91 0-1.1.43-1.1 1.07v1.4h2.17l-.28 2.2h-1.88v5.52a7.77 7.77 0 0 0 6.7-7.71" fill="#A8A8A8"></path></svg></span></button></div></div></div><div class="hy l fr"><div><div aria-hidden="false" class="ci"><button aria-label="Share on linkedin" class="au av aw ax ay az ba bb bc bd be bf bg bh bi"><span class="ci hz dw ia"><svg fill="none" height="24" viewbox="0 0 24 24" width="24"><path d="M19.75 5.39v13.22a1.14 1.14 0 0 1-1.14 1.14H5.39a1.14 1.14 0 0 1-1.14-1.14V5.39a1.14 1.14 0 0 1 1.14-1.14h13.22a1.14 1.14 0 0 1 1.14 1.14zM8.81 10.18H6.53v7.3H8.8v-7.3zM9 7.67a1.31 1.31 0 0 0-1.3-1.32h-.04a1.32 1.32 0 0 0 0 2.64A1.31 1.31 0 0 0 9 7.71v-.04zm8.46 5.37c0-2.2-1.4-3.05-2.78-3.05a2.6 2.6 0 0 0-2.3 1.18h-.07v-1h-2.14v7.3h2.28V13.6a1.51 1.51 0 0 1 1.36-1.63h.09c.72 0 1.26.45 1.26 1.6v3.91h2.28l.02-4.43z" fill="#A8A8A8"></path></svg></span></button></div></div></div><div class="l fr"><div><div aria-hidden="false" class="ci"><button class="au av aw ax ay az ba bb bc bd be bf bg bh bi"><span class="ci hz dw ia"><svg fill="none" height="24" viewbox="0 0 24 24" width="24"><path clip-rule="evenodd" d="M3.57 14.67c0-.57.13-1.11.38-1.6l.02-.02v-.02l.02-.02c0-.02 0-.02.02-.02.12-.26.3-.52.57-.8L7.78 9v-.02l.01-.02c.44-.41.91-.7 1.44-.85a4.87 4.87 0 0 0-1.19 2.36A5.04 5.04 0 0 0 8 11.6L6.04 13.6c-.19.19-.32.4-.38.65a2 2 0 0 0 0 .9c.08.2.2.4.38.57l1.29 1.31c.27.28.62.42 1.03.42.42 0 .78-.14 1.06-.42l1.23-1.25.79-.78 1.15-1.16c.08-.09.19-.22.28-.4.1-.2.15-.42.15-.67 0-.16-.02-.3-.06-.45l-.02-.02v-.02l-.07-.14s0-.03-.04-.06l-.06-.13-.02-.02c0-.02 0-.03-.02-.05a.6.6 0 0 0-.14-.16l-.48-.5c0-.04.02-.1.04-.15l.06-.12 1.17-1.14.09-.09.56.57c.02.04.08.1.16.18l.05.04.03.06.04.05.03.04.04.06.1.14.02.02c0 .02.01.03.03.04l.1.2v.02c.1.16.2.38.3.68a1 1 0 0 1 .04.25 3.2 3.2 0 0 1 .02 1.33 3.49 3.49 0 0 1-.95 1.87l-.66.67-.97.97-1.56 1.57a3.4 3.4 0 0 1-2.47 1.02c-.97 0-1.8-.34-2.49-1.03l-1.3-1.3a3.55 3.55 0 0 1-1-2.51v-.01h-.02v.02zm5.39-3.43c0-.19.02-.4.07-.63.13-.74.44-1.37.95-1.87l.66-.67.97-.98 1.56-1.56c.68-.69 1.5-1.03 2.47-1.03.97 0 1.8.34 2.48 1.02l1.3 1.32a3.48 3.48 0 0 1 1 2.48c0 .58-.11 1.11-.37 1.6l-.02.02v.02l-.02.04c-.14.27-.35.54-.6.8L16.23 15l-.01.02-.01.02c-.44.42-.92.7-1.43.83a4.55 4.55 0 0 0 1.23-3.52L18 10.38c.18-.21.3-.42.35-.65a2.03 2.03 0 0 0-.01-.9 1.96 1.96 0 0 0-.36-.58l-1.3-1.3a1.49 1.49 0 0 0-1.06-.42c-.42 0-.77.14-1.06.4l-1.2 1.27-.8.8-1.16 1.15c-.08.08-.18.21-.29.4a1.66 1.66 0 0 0-.08 1.12l.02.03v.02l.06.14s.01.03.05.06l.06.13.02.02.01.02.01.02c.05.08.1.13.14.16l.47.5c0 .04-.02.09-.04.15l-.06.12-1.15 1.15-.1.08-.56-.56a2.3 2.3 0 0 0-.18-.19c-.02-.01-.02-.03-.02-.04l-.02-.02a.37.37 0 0 1-.1-.12c-.03-.03-.05-.04-.05-.06l-.1-.15-.02-.02-.02-.04-.08-.17v-.02a5.1 5.1 0 0 1-.28-.69 1.03 1.03 0 0 1-.04-.26c-.06-.23-.1-.46-.1-.7v.01z" fill="#A8A8A8" fill-rule="evenodd"></path></svg></span></button></div></div></div><div class="ib o ao"><span><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fba70df6957d&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-ml-models-that-are-useless-ba70df6957d&amp;source=--------------------------bookmark_header-----------" rel="noopener follow"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" class="au de aw ax ay az ba hz bc id ie if ig"><svg aria-label="Add to list bookmark button" class="ic" fill="none" height="25" viewbox="0 0 25 25" width="25"><path d="M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18V2.5zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V7z" fill="#292929"></path></svg></button></a></span></div></div><div class="ck ih"><div><div aria-hidden="false" class="ci"></div></div></div></div></div><div class="ii ij ik j i d"><div class="fj l"><span><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fba70df6957d&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-ml-models-that-are-useless-ba70df6957d&amp;source=--------------------------bookmark_header-----------" rel="noopener follow"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" class="au de aw il ay az ba im bc id cd o ao in io ig"><svg aria-label="Add to list bookmark button" class="ic" fill="none" height="25" viewbox="0 0 25 25" width="25"><path d="M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18V2.5zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V7z" fill="#292929"></path></svg><p class="bm b bn bo cn">Save</p></button></a></span></div><div class="ip l fr"><div><div aria-hidden="false" class="ci"><button aria-label="Share on twitter" class="au av aw ax ay az ba bb bc bd be bf bg bh bi"><span class="ci hz dw ia"><svg fill="none" height="24" viewbox="0 0 24 24" width="24"><path d="M20 5.34c-.67.41-1.4.7-2.18.87a3.45 3.45 0 0 0-5.02-.1 3.49 3.49 0 0 0-1.02 2.47c0 .28.03.54.07.8a9.91 9.91 0 0 1-7.17-3.66 3.9 3.9 0 0 0-.5 1.74 3.6 3.6 0 0 0 1.56 2.92 3.36 3.36 0 0 1-1.55-.44V10c0 1.67 1.2 3.08 2.8 3.42-.3.06-.6.1-.94.12l-.62-.06a3.5 3.5 0 0 0 3.24 2.43 7.34 7.34 0 0 1-4.36 1.49l-.81-.05a9.96 9.96 0 0 0 5.36 1.56c6.4 0 9.91-5.32 9.9-9.9v-.5c.69-.49 1.28-1.1 1.74-1.81-.63.3-1.3.48-2 .56A3.33 3.33 0 0 0 20 5.33" fill="#A8A8A8"></path></svg></span></button></div></div></div><div class="ip l fr"><div><div aria-hidden="false" class="ci"><button aria-label="Share on facebook" class="au av aw ax ay az ba bb bc bd be bf bg bh bi"><span class="ci hz dw ia"><svg fill="none" height="24" viewbox="0 0 24 24" width="24"><path d="M19.75 12.04c0-4.3-3.47-7.79-7.75-7.79a7.77 7.77 0 0 0-5.9 12.84 7.77 7.77 0 0 0 4.69 2.63v-5.49h-1.9v-2.2h1.9v-1.62c0-1.88 1.14-2.9 2.8-2.9.8 0 1.49.06 1.69.08v1.97h-1.15c-.91 0-1.1.43-1.1 1.07v1.4h2.17l-.28 2.2h-1.88v5.52a7.77 7.77 0 0 0 6.7-7.71" fill="#A8A8A8"></path></svg></span></button></div></div></div><div class="ip l fr"><div><div aria-hidden="false" class="ci"><button aria-label="Share on linkedin" class="au av aw ax ay az ba bb bc bd be bf bg bh bi"><span class="ci hz dw ia"><svg fill="none" height="24" viewbox="0 0 24 24" width="24"><path d="M19.75 5.39v13.22a1.14 1.14 0 0 1-1.14 1.14H5.39a1.14 1.14 0 0 1-1.14-1.14V5.39a1.14 1.14 0 0 1 1.14-1.14h13.22a1.14 1.14 0 0 1 1.14 1.14zM8.81 10.18H6.53v7.3H8.8v-7.3zM9 7.67a1.31 1.31 0 0 0-1.3-1.32h-.04a1.32 1.32 0 0 0 0 2.64A1.31 1.31 0 0 0 9 7.71v-.04zm8.46 5.37c0-2.2-1.4-3.05-2.78-3.05a2.6 2.6 0 0 0-2.3 1.18h-.07v-1h-2.14v7.3h2.28V13.6a1.51 1.51 0 0 1 1.36-1.63h.09c.72 0 1.26.45 1.26 1.6v3.91h2.28l.02-4.43z" fill="#A8A8A8"></path></svg></span></button></div></div></div><div class="l fr"><div><div aria-hidden="false" class="ci"><button class="au av aw ax ay az ba bb bc bd be bf bg bh bi"><span class="ci hz dw ia"><svg fill="none" height="24" viewbox="0 0 24 24" width="24"><path clip-rule="evenodd" d="M3.57 14.67c0-.57.13-1.11.38-1.6l.02-.02v-.02l.02-.02c0-.02 0-.02.02-.02.12-.26.3-.52.57-.8L7.78 9v-.02l.01-.02c.44-.41.91-.7 1.44-.85a4.87 4.87 0 0 0-1.19 2.36A5.04 5.04 0 0 0 8 11.6L6.04 13.6c-.19.19-.32.4-.38.65a2 2 0 0 0 0 .9c.08.2.2.4.38.57l1.29 1.31c.27.28.62.42 1.03.42.42 0 .78-.14 1.06-.42l1.23-1.25.79-.78 1.15-1.16c.08-.09.19-.22.28-.4.1-.2.15-.42.15-.67 0-.16-.02-.3-.06-.45l-.02-.02v-.02l-.07-.14s0-.03-.04-.06l-.06-.13-.02-.02c0-.02 0-.03-.02-.05a.6.6 0 0 0-.14-.16l-.48-.5c0-.04.02-.1.04-.15l.06-.12 1.17-1.14.09-.09.56.57c.02.04.08.1.16.18l.05.04.03.06.04.05.03.04.04.06.1.14.02.02c0 .02.01.03.03.04l.1.2v.02c.1.16.2.38.3.68a1 1 0 0 1 .04.25 3.2 3.2 0 0 1 .02 1.33 3.49 3.49 0 0 1-.95 1.87l-.66.67-.97.97-1.56 1.57a3.4 3.4 0 0 1-2.47 1.02c-.97 0-1.8-.34-2.49-1.03l-1.3-1.3a3.55 3.55 0 0 1-1-2.51v-.01h-.02v.02zm5.39-3.43c0-.19.02-.4.07-.63.13-.74.44-1.37.95-1.87l.66-.67.97-.98 1.56-1.56c.68-.69 1.5-1.03 2.47-1.03.97 0 1.8.34 2.48 1.02l1.3 1.32a3.48 3.48 0 0 1 1 2.48c0 .58-.11 1.11-.37 1.6l-.02.02v.02l-.02.04c-.14.27-.35.54-.6.8L16.23 15l-.01.02-.01.02c-.44.42-.92.7-1.43.83a4.55 4.55 0 0 0 1.23-3.52L18 10.38c.18-.21.3-.42.35-.65a2.03 2.03 0 0 0-.01-.9 1.96 1.96 0 0 0-.36-.58l-1.3-1.3a1.49 1.49 0 0 0-1.06-.42c-.42 0-.77.14-1.06.4l-1.2 1.27-.8.8-1.16 1.15c-.08.08-.18.21-.29.4a1.66 1.66 0 0 0-.08 1.12l.02.03v.02l.06.14s.01.03.05.06l.06.13.02.02.01.02.01.02c.05.08.1.13.14.16l.47.5c0 .04-.02.09-.04.15l-.06.12-1.15 1.15-.1.08-.56-.56a2.3 2.3 0 0 0-.18-.19c-.02-.01-.02-.03-.02-.04l-.02-.02a.37.37 0 0 1-.1-.12c-.03-.03-.05-.04-.05-.06l-.1-.15-.02-.02-.02-.04-.08-.17v-.02a5.1 5.1 0 0 1-.28-.69 1.03 1.03 0 0 1-.04-.26c-.06-.23-.1-.46-.1-.7v.01z" fill="#A8A8A8" fill-rule="evenodd"></path></svg></span></button></div></div></div></div></header><span class="l"></span><section><div><div class="fo as iv iw ix iy"></div><div class="iz ja jb jc jd"><div class=""><h1 class="pw-post-title je jf jg bm jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc ga" id="22ce">Building ML Models That Are Useless</h1></div><div class=""><h2 class="pw-subtitle-paragraph kd jf jg bm b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku cn" id="c40e">Lessons from the Healthcare Industry</h2></div><figure class="kw kx ky kz gx la gl gm paragraph-image"><div class="lb lc do ld ce le" role="button" tabindex="0"><div class="gl gm kv"><picture><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/max/640/0*cvhSa_2KcctJf2oc 640w, https://miro.medium.com/max/720/0*cvhSa_2KcctJf2oc 720w, https://miro.medium.com/max/750/0*cvhSa_2KcctJf2oc 750w, https://miro.medium.com/max/786/0*cvhSa_2KcctJf2oc 786w, https://miro.medium.com/max/828/0*cvhSa_2KcctJf2oc 828w, https://miro.medium.com/max/1100/0*cvhSa_2KcctJf2oc 1100w, https://miro.medium.com/max/1400/0*cvhSa_2KcctJf2oc 1400w"/><img alt="" class="ce lf lg c" height="499" loading="eager" role="presentation" width="700"/></picture></div></div><figcaption class="lh bl gn gl gm li lj bm b bn bo cn">Pictured: Possibly a machine learning model. Kind of looks like it might be running somewhere in a hospital right?? Photo by <a class="au lk" href="https://unsplash.com/@kmuza?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Carlos Muza</a> on <a class="au lk" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p class="pw-post-body-paragraph ll lm jg ln b lo lp kh lq lr ls kk lt lu lv lw lx ly lz ma mb mc md me mf mg iz ga" id="4fc2">So you built an ML Model. Nice! Is it actually going to work? Not just in the Kaggle leaderboard sense, but in the literal staking-lives-on-this sense? The difference between these two was resoundingly demonstrated in a 2021 Nature paper [<a class="au lk" href="https://www.nature.com/articles/s42256-021-00338-7" rel="noopener ugc nofollow" target="_blank">1</a>] showing that, of all the recently published Covid-19 diagnostic AI models, none of them strongly generalized to datasets outside of those they were trained on. In other words, although their leaderboard scores were nearly perfect, if any of them had actually been deployed in a hospital it would have been a mess.</p><p class="pw-post-body-paragraph ll lm jg ln b lo lp kh lq lr ls kk lt lu lv lw lx ly lz ma mb mc md me mf mg iz ga" id="b500">This problem is near-and-dear to me since I work in the healthcare industry, and in that capacity I collaborated on the development of one such Covid-19 diagnostic AI model back in 2020. We never took our model to clinical deployment, as easier tests have since been developed that don’t require taking an X-Ray of everyone who comes into the clinic, but most of the development time spent on that model was spent looking for ways to avoid the kinds of generalization failures highlighted in the Nature paper.</p><figure class="kw kx ky kz gx la gl gm paragraph-image"><div class="lb lc do ld ce le" role="button" tabindex="0"><div class="gl gm mh"><picture><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/max/640/0*7CgConB42dVMryKo 640w, https://miro.medium.com/max/720/0*7CgConB42dVMryKo 720w, https://miro.medium.com/max/750/0*7CgConB42dVMryKo 750w, https://miro.medium.com/max/786/0*7CgConB42dVMryKo 786w, https://miro.medium.com/max/828/0*7CgConB42dVMryKo 828w, https://miro.medium.com/max/1100/0*7CgConB42dVMryKo 1100w, https://miro.medium.com/max/1400/0*7CgConB42dVMryKo 1400w"/><img alt="" class="ce lf lg c" height="821" loading="lazy" role="presentation" width="700"/></picture></div></div><figcaption class="lh bl gn gl gm li lj bm b bn bo cn">Pictured: Not the easiest way to figure out who has Covid-19. Photo by <a class="au lk" href="https://unsplash.com/@nci?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">National Cancer Institute</a> on <a class="au lk" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p class="pw-post-body-paragraph ll lm jg ln b lo lp kh lq lr ls kk lt lu lv lw lx ly lz ma mb mc md me mf mg iz ga" id="cad9">So how do you know whether your model really works, or whether your model just looks like it works? The authors in [<a class="au lk" href="https://www.nature.com/articles/s42256-021-00338-7" rel="noopener ugc nofollow" target="_blank">1</a>] argue that explainable AI (XAI) techniques are necessary in order to truly validate a model. I agree with the sentiment, though common XAI techniques can actually come with their own set of traps [<a class="au lk" href="https://arxiv.org/pdf/1810.03292.pdf" rel="noopener ugc nofollow" target="_blank">2</a>][<a class="au lk" href="https://arxiv.org/pdf/1811.10154.pdf" rel="noopener ugc nofollow" target="_blank">3</a>] which deserve a separate post. For this post, though, I want to focus on something more fundamental:</p><blockquote class="mi mj mk"><p class="ll lm ml ln b lo lp kh lq lr ls kk lt mm lv lw lx mn lz ma mb mo md me mf mg iz ga" id="d6d1">If you want to know if your model actually works, you need to test it on (multiple) totally different datasets than you trained it on.</p></blockquote><p class="pw-post-body-paragraph ll lm jg ln b lo lp kh lq lr ls kk lt lu lv lw lx ly lz ma mb mc md me mf mg iz ga" id="0d44">This doesn’t mean just taking your dataset and splitting off 20% for periodic evaluations and 10% for final testing. What I am suggesting is finding a set of independently collected samples and using them as a second (or third or fourth) dataset to validate your model. Obviously this is easier said than done — even in industrial scale companies with plenty of resources at their disposal. Aside from the obvious problem of locating this extra data, you’ll also have to maintain multiple data pipelines to manage it all (since these different datasets may require different pre-processing/normalization steps in order to get the inputs ready for your model).</p><p class="pw-post-body-paragraph ll lm jg ln b lo lp kh lq lr ls kk lt lu lv lw lx ly lz ma mb mc md me mf mg iz ga" id="2498">The data management part of this problem can at least be made easier through some specific tools, so for the rest of this post let’s walk through a few code examples to accomplish this, which also illustrate some of the unanticipated benefits you can reap while developing with generalizability in mind.</p></div><div class="o dx mp mq ii mr" role="separator"><span class="ms fl ci mt mu mv"></span><span class="ms fl ci mt mu mv"></span><span class="ms fl ci mt mu"></span></div><div class="iz ja jb jc jd"><figure class="kw kx ky kz gx la gl gm paragraph-image"><div class="lb lc do ld ce le" role="button" tabindex="0"><div class="gl gm mw"><picture><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/max/640/0*H6fm6o1Z8DSBU2MV 640w, https://miro.medium.com/max/720/0*H6fm6o1Z8DSBU2MV 720w, https://miro.medium.com/max/750/0*H6fm6o1Z8DSBU2MV 750w, https://miro.medium.com/max/786/0*H6fm6o1Z8DSBU2MV 786w, https://miro.medium.com/max/828/0*H6fm6o1Z8DSBU2MV 828w, https://miro.medium.com/max/1100/0*H6fm6o1Z8DSBU2MV 1100w, https://miro.medium.com/max/1400/0*H6fm6o1Z8DSBU2MV 1400w"/><img alt="" class="ce lf lg c" height="469" loading="eager" role="presentation" width="700"/></picture></div></div><figcaption class="lh bl gn gl gm li lj bm b bn bo cn">Pictured: A pretty good number, all things considered. Photo by <a class="au lk" href="https://unsplash.com/@marcel_eberle?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Marcel Eberle</a> on <a class="au lk" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p class="pw-post-body-paragraph ll lm jg ln b lo lp kh lq lr ls kk lt lu lv lw lx ly lz ma mb mc md me mf mg iz ga" id="395f">Let’s make a neural network that classifies numbers. Why? Because there are a bunch of datasets for this so it should be pretty easy to illustrate the main point. We’ll use the <a class="au lk" href="https://www.fastestimator.org" rel="noopener ugc nofollow" target="_blank">FastEstimator</a> (FE) framework since it works with both <a class="au lk" href="https://www.tensorflow.org" rel="noopener ugc nofollow" target="_blank">TensorFlow</a> and <a class="au lk" href="https://pytorch.org" rel="noopener ugc nofollow" target="_blank">PyTorch</a>, and I don’t know which way you swing. But also because I helped write FE to try and make this and other industrial-flavor use-cases easier.</p><p class="pw-post-body-paragraph ll lm jg ln b lo lp kh lq lr ls kk lt lu lv lw lx ly lz ma mb mc md me mf mg iz ga" id="feac">Time for the key steps of the traditional ML workflow (without following our new golden rule):</p><ol class=""><li class="mx my jg ln b lo lp lr ls lu mz ly na mc nb mg nc nd ne nf ga" id="2983">Find your Dataset. Let’s use USPS [<a class="au lk" href="https://ieeexplore.ieee.org/document/291440" rel="noopener ugc nofollow" target="_blank">4</a>] since it has some numbers in it.</li><li class="mx my jg ln b lo ng lr nh lu ni ly nj mc nk mg nc nd ne nf ga" id="ee37">Design your model. This isn’t the point today, so let’s just use ResNet9 [<a class="au lk" href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf" rel="noopener ugc nofollow" target="_blank">5</a>].</li><li class="mx my jg ln b lo ng lr nh lu ni ly nj mc nk mg nc nd ne nf ga" id="be45">Train your model.</li><li class="mx my jg ln b lo ng lr nh lu ni ly nj mc nk mg nc nd ne nf ga" id="21cc">Publish a paper on your groundbreaking results.</li></ol><p class="pw-post-body-paragraph ll lm jg ln b lo lp kh lq lr ls kk lt lu lv lw lx ly lz ma mb mc md me mf mg iz ga" id="2693">To save some time, here’s the code for Steps 1–3:</p><figure class="kw kx ky kz gx la"><div class="m fs l do"><div class="nl nm l"></div></div><figcaption class="lh bl gn gl gm li lj bm b bn bo cn">Nothing too special here yet</figcaption></figure><p class="pw-post-body-paragraph ll lm jg ln b lo lp kh lq lr ls kk lt lu lv lw lx ly lz ma mb mc md me mf mg iz ga" id="cb72">Now we just need to run our training and see how we did:</p><figure class="kw kx ky kz gx la gl gm paragraph-image"><div class="lb lc do ld ce le" role="button" tabindex="0"><div class="gl gm nn"><picture><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/max/640/1*9fKfIR0-y3UR8eL_vqN6zA.png 640w, https://miro.medium.com/max/720/1*9fKfIR0-y3UR8eL_vqN6zA.png 720w, https://miro.medium.com/max/750/1*9fKfIR0-y3UR8eL_vqN6zA.png 750w, https://miro.medium.com/max/786/1*9fKfIR0-y3UR8eL_vqN6zA.png 786w, https://miro.medium.com/max/828/1*9fKfIR0-y3UR8eL_vqN6zA.png 828w, https://miro.medium.com/max/1100/1*9fKfIR0-y3UR8eL_vqN6zA.png 1100w, https://miro.medium.com/max/1400/1*9fKfIR0-y3UR8eL_vqN6zA.png 1400w"/><img alt="" class="ce lf lg c" height="486" loading="lazy" role="presentation" width="700"/></picture></div></div><figcaption class="lh bl gn gl gm li lj bm b bn bo cn">USPS Validation Performance (Mathews Correlation Coefficient) vs Time. Mean +- Std. Dev over 5 runs. (Image by Author)</figcaption></figure><p class="pw-post-body-paragraph ll lm jg ln b lo lp kh lq lr ls kk lt lu lv lw lx ly lz ma mb mc md me mf mg iz ga" id="b325">Looks like we’re not doing too badly, with a final average test performance of 0.968. Let’s pause for a moment to dwell on two quick things here though. First, you always run multiple trials to get standard deviations right? Second, you’re not still using accuracy to measure your classification performance, right[<a class="au lk" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6941312/" rel="noopener ugc nofollow" target="_blank">6</a>]?? Since the Mathews Correlation Coefficient (MCC) is criminally underrated, let’s highlight the point for anyone just skimming through.</p><blockquote class="mi mj mk"><p class="ll lm ml ln b lo lp kh lq lr ls kk lt mm lv lw lx mn lz ma mb mo md me mf mg iz ga" id="9415">If you are using Accuracy or F1 score to validate your models, you should be using MCC instead.</p></blockquote><p class="pw-post-body-paragraph ll lm jg ln b lo lp kh lq lr ls kk lt lu lv lw lx ly lz ma mb mc md me mf mg iz ga" id="5c37">In this case our testing dataset has a balanced class distribution, so our MCC and Accuracy values are basically the same (0.968 vs 0.971). Nonetheless, building good habits is important since real datasets are basically never balanced, especially in the healthcare domain. MCC will keep you from accidentally fooling yourself when 99% of your data is from healthy patients and your model has an amazing 99% accuracy.</p><p class="pw-post-body-paragraph ll lm jg ln b lo lp kh lq lr ls kk lt lu lv lw lx ly lz ma mb mc md me mf mg iz ga" id="2fe3">Anyways, now we’re ready for Step 4 right? Not quite. Let’s apply our first golden rule from earlier, testing our model on some totally different datasets, just to be sure. How about MNIST [<a class="au lk" href="http://yann.lecun.com/exdb/mnist/" rel="noopener ugc nofollow" target="_blank">7</a>] and the digits dataset provided by Scikit-Learn (hereafter SKL) [<a class="au lk" href="https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits" rel="noopener ugc nofollow" target="_blank">8</a>]. I hear those have some numbers in them… FE will make it easy to keep things relatively clean:</p><figure class="kw kx ky kz gx la"><div class="m fs l do"><div class="nl nm l"></div></div><figcaption class="lh bl gn gl gm li lj bm b bn bo cn">A few lines of extra effort can go a long way</figcaption></figure><p class="pw-post-body-paragraph ll lm jg ln b lo lp kh lq lr ls kk lt lu lv lw lx ly lz ma mb mc md me mf mg iz ga" id="e9e4">Notice how adding these extra datasets only required small changes to the pipeline in order to load and wrangle all of the data into the same size, while shared transformations like MinMax can remain as they were. Any metrics you choose to use will automatically be computed for each dataset individually, as well as across all of the datasets put together. Let’s see how well our model really performs now that we have some extra digit datasets in the mix:</p><figure class="kw kx ky kz gx la gl gm paragraph-image"><div class="lb lc do ld ce le" role="button" tabindex="0"><div class="gl gm no"><picture><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/max/640/1*fmnLtVWudIZQ6XcA9z9Y2g.png 640w, https://miro.medium.com/max/720/1*fmnLtVWudIZQ6XcA9z9Y2g.png 720w, https://miro.medium.com/max/750/1*fmnLtVWudIZQ6XcA9z9Y2g.png 750w, https://miro.medium.com/max/786/1*fmnLtVWudIZQ6XcA9z9Y2g.png 786w, https://miro.medium.com/max/828/1*fmnLtVWudIZQ6XcA9z9Y2g.png 828w, https://miro.medium.com/max/1100/1*fmnLtVWudIZQ6XcA9z9Y2g.png 1100w, https://miro.medium.com/max/1400/1*fmnLtVWudIZQ6XcA9z9Y2g.png 1400w"/><img alt="" class="ce lf lg c" height="488" loading="lazy" role="presentation" width="700"/></picture></div></div><figcaption class="lh bl gn gl gm li lj bm b bn bo cn">Training summary with multiple datasets. Mean +- Std. Dev over 5 runs. (Image by Author)</figcaption></figure><p class="pw-post-body-paragraph ll lm jg ln b lo lp kh lq lr ls kk lt lu lv lw lx ly lz ma mb mc md me mf mg iz ga" id="2883">As you can see, even though our network is pretty good on the USPS dataset (0.97), its performance on MNIST (0.78) and SKL (0.66) is pretty bad. When we consider all the datasets together our MCC is only around 0.80. For those of you who aren’t comfortable with MCC yet, the accuracies here virtually identical: 0.97, 0.79, 0.67, and 0.80 respectively (both MCC and accuracy cap at 1.0, but MCC tends to penalize you faster as you move away from perfect performance). In any case, whereas before we might have concluded that our network understands numbers pretty well, we now know that it really only understands USPS-style numbers. This definitely isn’t what we set out wanting to accomplish.</p></div><div class="o dx mp mq ii mr" role="separator"><span class="ms fl ci mt mu mv"></span><span class="ms fl ci mt mu mv"></span><span class="ms fl ci mt mu"></span></div><div class="iz ja jb jc jd"><figure class="kw kx ky kz gx la gl gm paragraph-image"><div class="lb lc do ld ce le" role="button" tabindex="0"><div class="gl gm np"><picture><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/max/640/0*UY37OldlpNfuTIdT 640w, https://miro.medium.com/max/720/0*UY37OldlpNfuTIdT 720w, https://miro.medium.com/max/750/0*UY37OldlpNfuTIdT 750w, https://miro.medium.com/max/786/0*UY37OldlpNfuTIdT 786w, https://miro.medium.com/max/828/0*UY37OldlpNfuTIdT 828w, https://miro.medium.com/max/1100/0*UY37OldlpNfuTIdT 1100w, https://miro.medium.com/max/1400/0*UY37OldlpNfuTIdT 1400w"/><img alt="" class="ce lf lg c" height="467" loading="eager" role="presentation" width="700"/></picture></div></div><figcaption class="lh bl gn gl gm li lj bm b bn bo cn">This raises questions. Photo by <a class="au lk" href="https://unsplash.com/@evan__bray?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Evan Dennis</a> on <a class="au lk" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p class="pw-post-body-paragraph ll lm jg ln b lo lp kh lq lr ls kk lt lu lv lw lx ly lz ma mb mc md me mf mg iz ga" id="59af">So what do we do? Our model turned out to be pretty useless. Maybe 80% doesn’t sound too bad, but if the postal service used our model to parse 5-digit zip codes, only 33% of the mail would get where it was supposed to go. Unless we’re starting a Failure-As-A-Service company, we’ll have to make some improvements.</p><p class="pw-post-body-paragraph ll lm jg ln b lo lp kh lq lr ls kk lt lu lv lw lx ly lz ma mb mc md me mf mg iz ga" id="3c4b">One option would be to train on our MNIST and SKL datasets too. While this would be an easy way to improve performance on those specific datasets, then we would have to go out and find even more digit datasets to adequately test our generalization performance. That doesn’t sound very exciting, so let’s try something else instead.</p><p class="pw-post-body-paragraph ll lm jg ln b lo lp kh lq lr ls kk lt lu lv lw lx ly lz ma mb mc md me mf mg iz ga" id="9a0a">Back in NeurIPS 2019, researchers from the US Army published a paper [<a class="au lk" href="https://papers.nips.cc/paper/2019/file/cd61a580392a70389e27b0bc2b439f49-Paper.pdf" rel="noopener ugc nofollow" target="_blank">9</a>] proposing an alternative to the softmax layer of neural networks. Instead of using a softmax layer with a one-hot class encoding, they showed that neural networks can be made more robust against adversarial attacks simply by switching to a tanh output layer with classes being encoded using hadamard labels (though strong attacks can still break this defense [<a class="au lk" href="https://arxiv.org/pdf/2002.08347.pdf" rel="noopener ugc nofollow" target="_blank">10</a>]). The details of this are super interesting, so I highly encourage you to check out the paper. This post isn’t a summary of their paper though, I’m just curious whether the method can also help us improve our generalization performance (adversarially robust models have been shown to have more interpretable feature maps [<a class="au lk" href="https://arxiv.org/pdf/2002.08347.pdf" rel="noopener ugc nofollow" target="_blank">10</a>], which might in turn lead to better generalization). To test it we just need to make a slight modification to our final network layers, and add one additional pipeline pre-processing step to convert our class labels into a hadamard code representation.</p><figure class="kw kx ky kz gx la"><div class="m fs l do"><div class="nl nm l"></div></div><figcaption class="lh bl gn gl gm li lj bm b bn bo cn">Here I’m stitching changes on top of a TensorFlow model, but you could use PyTorch instead. FE handles both.</figcaption></figure><p class="pw-post-body-paragraph ll lm jg ln b lo lp kh lq lr ls kk lt lu lv lw lx ly lz ma mb mc md me mf mg iz ga" id="ed1e">Although the authors didn’t advertise this as a way to improve model generalization, let’s see how we did:</p><figure class="kw kx ky kz gx la gl gm paragraph-image"><div class="lb lc do ld ce le" role="button" tabindex="0"><div class="gl gm no"><picture><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/max/640/1*vGKaLn44UOlyWXs7lW0Byg.png 640w, https://miro.medium.com/max/720/1*vGKaLn44UOlyWXs7lW0Byg.png 720w, https://miro.medium.com/max/750/1*vGKaLn44UOlyWXs7lW0Byg.png 750w, https://miro.medium.com/max/786/1*vGKaLn44UOlyWXs7lW0Byg.png 786w, https://miro.medium.com/max/828/1*vGKaLn44UOlyWXs7lW0Byg.png 828w, https://miro.medium.com/max/1100/1*vGKaLn44UOlyWXs7lW0Byg.png 1100w, https://miro.medium.com/max/1400/1*vGKaLn44UOlyWXs7lW0Byg.png 1400w"/><img alt="" class="ce lf lg c" height="522" loading="lazy" role="presentation" width="700"/></picture></div></div><figcaption class="lh bl gn gl gm li lj bm b bn bo cn">Pro Tip: Having multiple graphs is a great way to impress management. (Image by Author)</figcaption></figure><p class="pw-post-body-paragraph ll lm jg ln b lo lp kh lq lr ls kk lt lu lv lw lx ly lz ma mb mc md me mf mg iz ga" id="35e6">Looks like we’ve got ourselves some clear takeaways:</p><ol class=""><li class="mx my jg ln b lo lp lr ls lu mz ly na mc nb mg nc nd ne nf ga" id="4bcd"><strong class="ln jh"><em class="ml">Our model is still technically useless to the postal service </em></strong>(you knew the title when you clicked)</li><li class="mx my jg ln b lo ng lr nh lu ni ly nj mc nk mg nc nd ne nf ga" id="8712">If we only consider USPS performance, it would be unclear that the introduction of hadamard codes caused any improvement</li><li class="mx my jg ln b lo ng lr nh lu ni ly nj mc nk mg nc nd ne nf ga" id="3008">Introducing hadamard codes actually did significantly improve our best MCC when considering all the datasets together</li></ol><p class="pw-post-body-paragraph ll lm jg ln b lo lp kh lq lr ls kk lt lu lv lw lx ly lz ma mb mc md me mf mg iz ga" id="61d3">To see point 2 more clearly, let’s zoom in on the USPS performance graph:</p><figure class="kw kx ky kz gx la gl gm paragraph-image"><div class="lb lc do ld ce le" role="button" tabindex="0"><div class="gl gm nq"><picture><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/max/640/1*9Q3T68qMoqSMqZDwcI5BHw.png 640w, https://miro.medium.com/max/720/1*9Q3T68qMoqSMqZDwcI5BHw.png 720w, https://miro.medium.com/max/750/1*9Q3T68qMoqSMqZDwcI5BHw.png 750w, https://miro.medium.com/max/786/1*9Q3T68qMoqSMqZDwcI5BHw.png 786w, https://miro.medium.com/max/828/1*9Q3T68qMoqSMqZDwcI5BHw.png 828w, https://miro.medium.com/max/1100/1*9Q3T68qMoqSMqZDwcI5BHw.png 1100w, https://miro.medium.com/max/1400/1*9Q3T68qMoqSMqZDwcI5BHw.png 1400w"/><img alt="" class="ce lf lg c" height="530" loading="lazy" role="presentation" width="700"/></picture></div></div><figcaption class="lh bl gn gl gm li lj bm b bn bo cn">As before, orange is hadamard and blue is baseline. (Image by Author)</figcaption></figure><p class="pw-post-body-paragraph ll lm jg ln b lo lp kh lq lr ls kk lt lu lv lw lx ly lz ma mb mc md me mf mg iz ga" id="f25e">If this was all the data that you had, you might say that hadamard is showing some initial promise, but in the end the test set performances were within 1 standard deviation of each other (0.967 baseline vs 0.973 hadamard).</p><blockquote class="mi mj mk"><p class="ll lm ml ln b lo lp kh lq lr ls kk lt mm lv lw lx mn lz ma mb mo md me mf mg iz ga" id="e482">Since most people develop models considering only one dataset at a time, they would probably stop here and revert their changes without noticing the generalization benefits of the method.</p></blockquote><p class="pw-post-body-paragraph ll lm jg ln b lo lp kh lq lr ls kk lt lu lv lw lx ly lz ma mb mc md me mf mg iz ga" id="291a">Once we start looking at the performances on MNIST and SKL however, we can see a very different story. The generalization performance of the hadamard model is noticeably higher than that of the baseline architecture. This is reflected in the solid separation of the curves in the Combined MCC graph (the max overall MCC increased from 0.80 to 0.88, with MNIST peak performance rising from 0.77 to 0.84 and SKL peak performance rising from 0.66 to 0.82). Hadamard codes didn’t fully close our generalization gap, but they did help, and maybe gave us some extra adversarial robustness for free. I’ll take it.</p></div><div class="o dx mp mq ii mr" role="separator"><span class="ms fl ci mt mu mv"></span><span class="ms fl ci mt mu mv"></span><span class="ms fl ci mt mu"></span></div><div class="iz ja jb jc jd"><p class="pw-post-body-paragraph ll lm jg ln b lo lp kh lq lr ls kk lt lu lv lw lx ly lz ma mb mc md me mf mg iz ga" id="3485">So what next? Seems like we’re stuck on Step 3 and everyone knows that Step 4 is where the fame and glory is. Well, we’ve already found one technique that boosted our overall score by 0.08. If we can just manage to do that one more time we’ll have ourselves an ML model that actually generalizes well (read: isn’t useless). Realistically, we probably do want a better dataset to pull this off. There’s a large one called DIDA [<a class="au lk" href="https://www.sciencedirect.com/science/article/pii/S2214579620300502?via%3Dihub" rel="noopener ugc nofollow" target="_blank">11</a>] from 2021 which could be worth taking a look at. That may not seem like a very satisfying conclusion, but hey, if I already had the perfect solution for model generalization I’d be the one working on Step 4 right now ;)</p><p class="pw-post-body-paragraph ll lm jg ln b lo lp kh lq lr ls kk lt lu lv lw lx ly lz ma mb mc md me mf mg iz ga" id="0c7c">Much more importantly, we now have a method in place to easily test the impact of model modifications on our generalization performance. As we saw, without monitoring this explicitly, we could easily fail to notice the true utility of ideas we’re experimenting with. Now that we have it, we can make better decisions and be much more confident that, when we do finally get a testing score we’re satisfied with, our model won’t mercilessly betray us in production. Fame and glory here we come!</p><figure class="kw kx ky kz gx la gl gm paragraph-image"><div class="lb lc do ld ce le" role="button" tabindex="0"><div class="gl gm nr"><picture><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/max/640/0*az1zjXshoBIdjWOm 640w, https://miro.medium.com/max/720/0*az1zjXshoBIdjWOm 720w, https://miro.medium.com/max/750/0*az1zjXshoBIdjWOm 750w, https://miro.medium.com/max/786/0*az1zjXshoBIdjWOm 786w, https://miro.medium.com/max/828/0*az1zjXshoBIdjWOm 828w, https://miro.medium.com/max/1100/0*az1zjXshoBIdjWOm 1100w, https://miro.medium.com/max/1400/0*az1zjXshoBIdjWOm 1400w"/><img alt="" class="ce lf lg c" height="468" loading="eager" role="presentation" width="700"/></picture></div></div><figcaption class="lh bl gn gl gm li lj bm b bn bo cn">They give trophies for best paper right? What?? Wait, what’s even the point then? Photo by <a class="au lk" href="https://unsplash.com/@tommaomaoer?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">tommao wang</a> on <a class="au lk" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure></div><div class="o dx mp mq ii mr" role="separator"><span class="ms fl ci mt mu mv"></span><span class="ms fl ci mt mu mv"></span><span class="ms fl ci mt mu"></span></div><div class="iz ja jb jc jd"><h1 class="ns nt jg bm nu nv nw nx ny nz oa ob oc km od kn oe kp of kq og ks oh kt oi oj ga" id="2bb8">References</h1><p class="pw-post-body-paragraph ll lm jg ln b lo ok kh lq lr ol kk lt lu om lw lx ly on ma mb mc oo me mf mg iz ga" id="87f0">[1] A. DeGrave, J. Janizek, and S. Lee, <a class="au lk" href="https://www.nature.com/articles/s42256-021-00338-7" rel="noopener ugc nofollow" target="_blank">AI for radiographic COVID-19 detection selects shortcuts over signal</a> (2021), Nature Machine Intelligence<br/>[2] J. Adebayo, J. Gilmer, M. Mulley, I. Goodfellow, M. Hardt, and B. Kim, <a class="au lk" href="https://arxiv.org/pdf/1810.03292.pdf" rel="noopener ugc nofollow" target="_blank">Sanity Checks for Saliency Maps</a> (2018), NeurIPS<br/>[3] C. Rudin, <a class="au lk" href="https://arxiv.org/pdf/1811.10154.pdf" rel="noopener ugc nofollow" target="_blank">Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead</a> (2018), NeurIPS Workshop<br/>[4] J. Hull, <a class="au lk" href="https://ieeexplore.ieee.org/document/291440" rel="noopener ugc nofollow" target="_blank">A database for handwritten text recognition research</a> (1994), IEEE<br/>[5] K. He, X. Zhang, S. Ren, and J. Sun, <a class="au lk" href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf" rel="noopener ugc nofollow" target="_blank">Deep Residual Learning for Image Recognition</a> (2016), CVPR<br/>[6] D. Chicco and G. Jurman, <a class="au lk" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6941312/" rel="noopener ugc nofollow" target="_blank">The advantages of the Mathews correlation coefficient (MCC) over F1 score and accuracy in binary classification evaluation</a> (2020), BMC Genomics<br/>[7] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, <a class="au lk" href="https://ieeexplore.ieee.org/document/726791" rel="noopener ugc nofollow" target="_blank">Gradient-based learning applied to document recognition</a> (1998), IEEE<br/>[8] C. Kaynak, <a class="au lk" href="https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits" rel="noopener ugc nofollow" target="_blank">Methods of Combining Multiple Classifiers and Their Applications to Handwritten Digit Recognition</a> (1995), MSc Thesis, Institute of Graduate Studies in Science and Engineering, Bogazici University.<br/>[9] G. Verma and A. Swami, <a class="au lk" href="https://papers.nips.cc/paper/2019/file/cd61a580392a70389e27b0bc2b439f49-Paper.pdf" rel="noopener ugc nofollow" target="_blank">Error Correcting Output Codes Improve Probability Estimation and Adversarial Robustness of Deep Neural Networks</a> (2019), NeurIPS<br/>[10] F. Tramèr, N. Carlini, W. Brendel, and A. Mary, <a class="au lk" href="https://proceedings.neurips.cc/paper/2020/file/11f38f8ecd71867b42433548d1078e38-Paper.pdf" rel="noopener ugc nofollow" target="_blank">On Adaptive Attacks to Adversarial Example Defenses</a> (2020), NeurIPS<br/>[11] H. Kusetogullari, A. Yavariabdi, J. Hall, and N. Lavesson, <a class="au lk" href="https://www.sciencedirect.com/science/article/pii/S2214579620300502?via%3Dihub" rel="noopener ugc nofollow" target="_blank">DIGITNET: A Deep Handwritten Digit Detection and Recognition Method Using a New Historical Handwritten Digit Dataset</a> (2021), Big Data Research</p></div></div></section></div></div></article>